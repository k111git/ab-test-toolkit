{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrappers\n",
    "\n",
    "> Wrappers to create showcases and function to explain concepts on a high level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.20.0.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# | hide\n",
    "from plotly.offline import init_notebook_mode\n",
    "\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:jupyter_black:config: {'line_length': 79, 'target_versions': {<TargetVersion.PY310: 10>}}\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            (function() {\n",
       "                jb_set_cell(\"# | export\\ndef plot_comparison_ate_pvalue(\\n    cr0=0.10, crmax=0.12, sizes=[1000, 2000, 5000, 10000], one_sided=True\\n):\\n    \\\"\\\"\\\"\\n    Compares the average treatment effect (difference in conversion rates) with the P value for different sample sizes.\\n    Inputs:\\n    sizes: List of total sample sizes considered\\n    cr0: baseline conversion rate\\n    crmax: max conversion rate (we will plot ATEs until crmax - cr0)\\n    Output: Plot\\n    \\\"\\\"\\\"\\n    dfs = []\\n    for size in sizes:\\n        pvs = []\\n        ates = []\\n        for cr1 in np.linspace(cr0, crmax, 100):\\n            df = generate_contingency(N=size, cr0=cr0, cr1=cr1, exact=True)\\n            this_ate = df.loc[1].cvr - df.loc[0].cvr\\n            this_pv = p_value_binary(df, one_sided=one_sided)\\n            pvs.append(this_pv)\\n            ates.append(this_ate)\\n        out = pd.DataFrame({\\\"ate\\\": ates, \\\"pvs\\\": pvs})\\n        out[\\\"size\\\"] = size\\n        dfs.append(out)\\n    out_all = pd.concat(dfs)\\n    fig = px.line(out_all, x=\\\"ate\\\", y=\\\"pvs\\\", color=\\\"size\\\")\\n    fig.update_layout(\\n        height=600,\\n        width=800,\\n        legend=dict(yanchor=\\\"top\\\", y=1, xanchor=\\\"right\\\", x=0.99),\\n    )\\n    # fig.update_xaxes(showspikes=True, spikemode=\\\"across\\\", spikethickness=1)\\n    # fig.update_yaxes(showspikes=True, spikemode=\\\"across\\\", spikethickness=1)\\n    # fig.update_layout(hoverdistance=100)\\n    return fig\")\n",
       "            })();\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| hide \n",
    "import black\n",
    "import jupyter_black\n",
    "\n",
    "jupyter_black.load(\n",
    "    lab=False,\n",
    "    line_length=79,\n",
    "    verbosity=\"DEBUG\",\n",
    "    target_version=black.TargetVersion.PY310,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from ab_test_toolkit.generator import (\n",
    "    generate_binary_data,\n",
    "    generate_continuous_data,\n",
    ")\n",
    "from ab_test_toolkit.power import (\n",
    "    simulate_power_binary,\n",
    "    sample_size_binary,\n",
    "    simulate_power_continuous,\n",
    "    sample_size_continuous,\n",
    ")\n",
    "from ab_test_toolkit.plotting import (\n",
    "    plot_power,\n",
    "    plot_distribution,\n",
    "    plot_betas,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ab_test_toolkit.generator import (\n",
    "    data_to_contingency,\n",
    "    generate_binary_data,\n",
    "    generate_contingency,\n",
    ")\n",
    "from ab_test_toolkit.analyze import p_value_binary\n",
    "import plotly.io as pio\n",
    "\n",
    "pio.templates.default = \"simple_white\"\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.figure_factory as ff\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def simulate_evolution_binary(\n",
    "    N=10000,\n",
    "    cr0=0.10,\n",
    "    cr1=0.11,\n",
    "    snapshot_sizes=np.arange(1000, 10001, 1000),\n",
    "    one_sided=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Simulates the evolution of a binary experiment.\n",
    "    \"\"\"\n",
    "    df = generate_binary_data(N=N, cr0=cr0, cr1=cr1)\n",
    "    plot_sizes = snapshot_sizes\n",
    "    result_dfs = []\n",
    "    for current_size in plot_sizes:\n",
    "        df1 = df[:current_size]\n",
    "        df_c = data_to_contingency(df1)\n",
    "        users0, users1 = df_c.users\n",
    "        converted0, converted1 = df_c.converted\n",
    "        rate0, rate1 = df_c.cvr\n",
    "        ate = rate1 - rate0\n",
    "        pv = p_value_binary(df_c, one_sided=one_sided)\n",
    "        out_df = pd.DataFrame(\n",
    "            {\n",
    "                \"size\": [current_size],\n",
    "                \"users0\": users0,\n",
    "                \"users1\": users1,\n",
    "                \"converted0\": converted0,\n",
    "                \"converted1\": converted1,\n",
    "                \"cr0\": rate0,\n",
    "                \"cr1\": rate1,\n",
    "                \"pv\": [pv],\n",
    "                \"ate\": [ate],\n",
    "            }\n",
    "        )\n",
    "        result_dfs.append(out_df)\n",
    "    result_df = pd.concat(result_dfs)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "result_df = simulate_evolution_binary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def realizations_of_evolution_binary(\n",
    "    N_realizations=50,\n",
    "    cr0=0.10,\n",
    "    cr1=0.11,\n",
    "    snapshot_sizes=np.arange(1000, 10001, 1000),\n",
    "    one_sided=True,\n",
    "):\n",
    "    N = np.max(snapshot_sizes)\n",
    "    realizations_df = []\n",
    "    for i in range(0, N_realizations):\n",
    "        result_df = simulate_evolution_binary(\n",
    "            N=N,\n",
    "            cr0=cr0,\n",
    "            cr1=cr1,\n",
    "            snapshot_sizes=snapshot_sizes,\n",
    "            one_sided=one_sided,\n",
    "        )\n",
    "        realizations_df.append(result_df)\n",
    "        if (i % 10) == 0:\n",
    "            print(f\"{i} done\")\n",
    "    print(\"all done\")\n",
    "\n",
    "    snapshots_df = [\n",
    "        pd.DataFrame(\n",
    "            [\n",
    "                {\n",
    "                    \"realization\": i,\n",
    "                    \"ate\": realizations_df[i].iloc[j].ate,\n",
    "                    \"pvalue\": realizations_df[i].iloc[j].pv,\n",
    "                }\n",
    "                for i in range(0, N_realizations)\n",
    "            ]\n",
    "        ).sort_values(by=\"ate\")\n",
    "        for j in range(0, len(snapshot_sizes))\n",
    "    ]\n",
    "    return {\n",
    "        \"dataframes\": realizations_df,\n",
    "        \"snapshots\": snapshots_df,\n",
    "        \"snapshot_sizes\": snapshot_sizes,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def plot_realization(\n",
    "    plot_df, multiply_ate=1.0, alpha=0.05, ate_line=0.01, info=False\n",
    "):\n",
    "    colors = [\"#1f77b4\", \"#ff7f0e\"]\n",
    "    colors_bottom=['#2ca02c','#d62728']\n",
    "    if info == False:\n",
    "        n_cols = 1\n",
    "        specs = [[{\"secondary_y\": False}]]\n",
    "        height = 400\n",
    "    else:\n",
    "        n_cols = 2\n",
    "        specs = [[{\"secondary_y\": False}], [{\"secondary_y\": True}]]\n",
    "        height = 700\n",
    "    fig = make_subplots(\n",
    "        rows=n_cols,\n",
    "        cols=1,\n",
    "        specs=specs,\n",
    "    )\n",
    "\n",
    "    if len(plot_df) >= 20:\n",
    "        mode = \"lines\"\n",
    "    else:\n",
    "        mode = \"lines+markers\"\n",
    "\n",
    "    for group in [0, 1]:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=plot_df[\"size\"],\n",
    "                y=plot_df[f\"converted{group}\"],\n",
    "                mode=mode,\n",
    "                name=f\"Group: {group}\",\n",
    "                line_color=colors[group],\n",
    "                legendgroup=\"1\",\n",
    "            ),\n",
    "            row=1,\n",
    "            col=1,\n",
    "        )\n",
    "\n",
    "    if info == True:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=plot_df[\"size\"],\n",
    "                y=plot_df[f\"pv\"],\n",
    "                mode=mode,\n",
    "                name=f\"pvalue\",\n",
    "                line_color=\"#ff7f0e\",\n",
    "                legendgroup=\"2\",\n",
    "            ),\n",
    "            row=2,\n",
    "            col=1,\n",
    "        )\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=plot_df[\"size\"],\n",
    "                y=[alpha] * len(plot_df[\"size\"]),\n",
    "                mode=\"lines\",\n",
    "                name=f\"p={alpha}\",\n",
    "                line_color=\"#ff7f0e\",\n",
    "                line_dash=\"dot\",\n",
    "                legendgroup=\"2\",\n",
    "            ),\n",
    "            row=2,\n",
    "            col=1,\n",
    "        )\n",
    "\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=plot_df[\"size\"],\n",
    "                y=multiply_ate * plot_df[f\"ate\"],\n",
    "                mode=mode,\n",
    "                name=f\"ate\",\n",
    "                line_color=colors_bottom[0],\n",
    "                legendgroup=\"2\",\n",
    "            ),\n",
    "            row=2,\n",
    "            col=1,\n",
    "            secondary_y=True,\n",
    "        )\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=plot_df[\"size\"],\n",
    "                y=[ate_line] * len(plot_df[\"size\"]),\n",
    "                mode=\"lines\",\n",
    "                name=f\"ate={ate_line}\",\n",
    "                line_color=colors_bottom[1],\n",
    "                line_dash=\"dot\",\n",
    "                legendgroup=\"2\",\n",
    "            ),\n",
    "            row=2,\n",
    "            col=1,\n",
    "            secondary_y=True,\n",
    "        )\n",
    "        fig.update_yaxes(\n",
    "            title_text=\"ate\",\n",
    "            secondary_y=True,\n",
    "            row=2,\n",
    "            col=1,\n",
    "        )\n",
    "\n",
    "        fig.update_yaxes(\n",
    "            title_text=\"pvalue\", secondary_y=False, row=2, col=1, range=[0, 1]\n",
    "        )\n",
    "\n",
    "    fig.update_yaxes(\n",
    "        title_text=\"Conversions\",\n",
    "        secondary_y=False,\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        height=height,\n",
    "        width=1000,\n",
    "        title_text=\"\",\n",
    "        legend_tracegroupgap=250,\n",
    "    )\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def plot_snapshots_distribution(\n",
    "    snapshots, vline_x=None, snapshot_indices=None\n",
    "):\n",
    "    if snapshot_indices == None:\n",
    "        snapshot_indices = [0, int(len(snapshots) / 2.0), len(snapshots) - 1]\n",
    "    fig = ff.create_distplot(\n",
    "        [snapshots[j][\"ate\"] for j in snapshot_indices],\n",
    "        snapshot_indices,\n",
    "        show_hist=False,\n",
    "        show_rug=True,\n",
    "    )\n",
    "    if vline_x != None:\n",
    "        fig.add_vline(\n",
    "            x=vline_x, line_width=1, line_dash=\"dash\", line_color=\"gray\"\n",
    "        )\n",
    "\n",
    "    fig.update_layout(\n",
    "        template=\"simple_white\",\n",
    "        xaxis_title=\"ATE\",\n",
    "        yaxis_title=\"PDF\",\n",
    "        legend=dict(yanchor=\"top\", y=0.99, xanchor=\"right\", x=0.95),\n",
    "        height=600,\n",
    "    )\n",
    "    #     fig.update_xaxes(range=[-0.04, 0.04])\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 done\n",
      "10 done\n",
      "20 done\n",
      "30 done\n",
      "40 done\n",
      "all done\n"
     ]
    }
   ],
   "source": [
    "# | hide\n",
    "r0 = realizations_of_evolution_binary(\n",
    "    N_realizations=50,\n",
    "    cr0=0.10,\n",
    "    cr1=0.10,\n",
    "    snapshot_sizes=np.arange(1000, 10001, 1000),\n",
    ")\n",
    "# plot_snapshots_distribution(r0[\"snapshots\"])\n",
    "# plot_realization(r0[\"dataframes\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def analytics_null_vs_effect(r0, r1, alpha=0.1, ate_limit=0.005):\n",
    "    \"\"\"\n",
    "    Shows the power and false positives of using a p value vs directly the ATE\n",
    "    r0,r1: Realization objects from realizations_of_evolution_binary\n",
    "    r0: No effect\n",
    "    r1: With effect\n",
    "    \"\"\"\n",
    "    ate = dict()\n",
    "    pv = dict()\n",
    "    for effect, snapshot in zip(\n",
    "        [\"null\", \"effect\"], [r0[\"snapshots\"], r1[\"snapshots\"]]\n",
    "    ):\n",
    "        ate[effect] = dict()\n",
    "        ate[effect][\"positives\"] = [\n",
    "            (df[\"ate\"] > ate_limit).mean() for df in snapshot\n",
    "        ]\n",
    "        ate[effect][\"negatives\"] = [\n",
    "            (df[\"ate\"] <= ate_limit).mean() for df in snapshot\n",
    "        ]\n",
    "        pv[effect] = dict()\n",
    "        pv[effect][\"positives\"] = [\n",
    "            (df[\"pvalue\"] < alpha).mean() for df in snapshot\n",
    "        ]\n",
    "        pv[effect][\"negatives\"] = [\n",
    "            (df[\"pvalue\"] >= alpha).mean() for df in snapshot\n",
    "        ]\n",
    "\n",
    "    fp = pv[\"null\"][\"positives\"][-1]\n",
    "    fn = pv[\"effect\"][\"negatives\"][-1]\n",
    "    tp = pv[\"effect\"][\"positives\"][-1]\n",
    "    tn = pv[\"null\"][\"negatives\"][-1]\n",
    "\n",
    "    confusion_p = pd.DataFrame({0: [tn, fp], 1: [fn, tp]}) / 2.0\n",
    "\n",
    "    fp = ate[\"null\"][\"positives\"][-1]\n",
    "    fn = ate[\"effect\"][\"negatives\"][-1]\n",
    "    tp = ate[\"effect\"][\"positives\"][-1]\n",
    "    tn = ate[\"null\"][\"negatives\"][-1]\n",
    "\n",
    "    confusion_ate = pd.DataFrame({0: [tn, fp], 1: [fn, tp]}) / 2.0\n",
    "\n",
    "    return {\n",
    "        \"ate\": ate,\n",
    "        \"pv\": pv,\n",
    "        \"confusion_p\": confusion_p,\n",
    "        \"confusion_ate\": confusion_ate,\n",
    "        \"alpha\": alpha,\n",
    "        \"snapshot_sizes\": r0[\"snapshot_sizes\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def plot_analytics(analytics):\n",
    "    \"\"\"\n",
    "    plots elements of confusion matrix over time\n",
    "    \"\"\"\n",
    "    alpha = analytics[\"alpha\"]\n",
    "\n",
    "    my_sizes = analytics[\"snapshot_sizes\"]\n",
    "\n",
    "    if len(my_sizes) >= 100:\n",
    "        label_every = 10\n",
    "    elif len(my_sizes) >= 30:\n",
    "        label_every = 4\n",
    "    elif len(my_sizes) >= 20:\n",
    "        label_every = 2\n",
    "    else:\n",
    "        label_every = 1\n",
    "\n",
    "    fig = make_subplots(\n",
    "        rows=2,\n",
    "        cols=1,\n",
    "        shared_xaxes=True,\n",
    "        subplot_titles=(\"No true effect\", \"With true effect\"),\n",
    "        vertical_spacing=0.1,\n",
    "    )\n",
    "    colors = [\"#1f77b4\", \"#ff7f0e\"]\n",
    "    for idx, appraoch in enumerate([\"ate\", \"pv\"]):\n",
    "        tn = analytics[appraoch][\"null\"][\"negatives\"]\n",
    "        fp = analytics[appraoch][\"null\"][\"positives\"]\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=np.array(range(0, len(tn))),\n",
    "                y=tn,\n",
    "                mode=\"lines\",\n",
    "                name=f\"{appraoch} TN\",\n",
    "                line_color=colors[idx],\n",
    "                legendgroup=\"1\",\n",
    "            ),\n",
    "            row=1,\n",
    "            col=1,\n",
    "        )\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=np.array(range(0, len(fp))),\n",
    "                y=fp,\n",
    "                mode=\"lines\",\n",
    "                name=f\"{appraoch} FP\",\n",
    "                line_color=colors[idx],\n",
    "                legendgroup=\"1\",\n",
    "                line=dict(dash=\"dash\"),\n",
    "            ),\n",
    "            row=1,\n",
    "            col=1,\n",
    "        )\n",
    "\n",
    "    for idx, appraoch in enumerate([\"ate\", \"pv\"]):\n",
    "        tn = analytics[appraoch][\"effect\"][\"negatives\"]\n",
    "        fp = analytics[appraoch][\"effect\"][\"positives\"]\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=np.array(range(0, len(tn))),\n",
    "                y=tn,\n",
    "                mode=\"lines\",\n",
    "                name=f\"{appraoch} FN\",\n",
    "                line_color=colors[idx],\n",
    "                legendgroup=\"2\",\n",
    "                line=dict(dash=\"dash\"),\n",
    "            ),\n",
    "            row=2,\n",
    "            col=1,\n",
    "        )\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=np.array(range(0, len(fp))),\n",
    "                y=fp,\n",
    "                mode=\"lines\",\n",
    "                name=f\"{appraoch} TP\",\n",
    "                line_color=colors[idx],\n",
    "                legendgroup=\"2\",\n",
    "            ),\n",
    "            row=2,\n",
    "            col=1,\n",
    "        )\n",
    "\n",
    "    fig.update_yaxes(\n",
    "        title_text=\"\",\n",
    "        row=2,\n",
    "        col=1,\n",
    "    )\n",
    "    fig.update_yaxes(\n",
    "        title_text=\"\",\n",
    "        row=1,\n",
    "        col=1,\n",
    "    )\n",
    "    fig.update_xaxes(\n",
    "        title_text=\"Time\",\n",
    "        row=2,\n",
    "        col=1,\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        height=800,\n",
    "        width=1000,\n",
    "        title_text=f\"Power and False Positives, alpha={alpha}\",\n",
    "        legend_tracegroupgap=350,\n",
    "    )\n",
    "\n",
    "    fig = fig.update_xaxes(\n",
    "        title_text=\"Size per variant\",\n",
    "        tickvals=np.array(range(0, len(my_sizes)))[::label_every],\n",
    "        ticktext=my_sizes[::label_every]\n",
    "        / 2.0,  # divide by 2 due to sample size per variant.\n",
    "        row=2,\n",
    "        col=1,\n",
    "    )\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def plot_analytics_compact(analytics, approach):\n",
    "    \"\"\"\n",
    "    plots elements of confusion matrix over time.\n",
    "    input: analytics element from analytics_null_vs_effect\n",
    "    approach: \"both\", \"ate\", or \"pv\".\n",
    "    \"\"\"\n",
    "    alpha = analytics[\"alpha\"]\n",
    "\n",
    "    my_sizes = analytics[\"snapshot_sizes\"]\n",
    "\n",
    "    if len(my_sizes) >= 100:\n",
    "        label_every = 10\n",
    "    elif len(my_sizes) >= 30:\n",
    "        label_every = 4\n",
    "    elif len(my_sizes) >= 20:\n",
    "        label_every = 2\n",
    "    else:\n",
    "        label_every = 1\n",
    "\n",
    "    fig = make_subplots()\n",
    "    colors = [\"#1f77b4\", \"#ff7f0e\"]\n",
    "\n",
    "    if approach != \"both\":\n",
    "        tp = analytics[approach][\"effect\"][\"positives\"]\n",
    "        fp = analytics[approach][\"null\"][\"positives\"]\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=np.array(range(0, len(fp))),\n",
    "                y=fp,\n",
    "                mode=\"lines\",\n",
    "                name=f\"False positives (alpha)\",\n",
    "                line_color=colors[0],\n",
    "            ),\n",
    "        )\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=np.array(range(0, len(tp))),\n",
    "                y=tp,\n",
    "                mode=\"lines\",\n",
    "                name=f\"True positives (power)\",\n",
    "                line_color=colors[1],\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        dasher = [\"solid\", \"dash\"]\n",
    "        for idx, this_approach in enumerate([\"ate\", \"pv\"]):\n",
    "            tp = analytics[this_approach][\"effect\"][\"positives\"]\n",
    "            fp = analytics[this_approach][\"null\"][\"positives\"]\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=np.array(range(0, len(fp))),\n",
    "                    y=fp,\n",
    "                    mode=\"lines\",\n",
    "                    name=f\"False positives {this_approach}\",\n",
    "                    line=dict(dash=dasher[idx]),\n",
    "                    line_color=colors[0],\n",
    "                ),\n",
    "            )\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=np.array(range(0, len(tp))),\n",
    "                    y=tp,\n",
    "                    mode=\"lines\",\n",
    "                    name=f\"True positives {this_approach}\",\n",
    "                    line=dict(dash=dasher[idx]),\n",
    "                    line_color=colors[1],\n",
    "                ),\n",
    "            )\n",
    "\n",
    "    fig.update_yaxes(title_text=\"\", range=[0, 1])\n",
    "    fig.update_xaxes(\n",
    "        title_text=\"Time\",\n",
    "    )\n",
    "\n",
    "    fig.update_layout(\n",
    "        height=600,\n",
    "        width=1000,\n",
    "        title_text=f\"Power and False Positives: {approach}\",\n",
    "        legend=dict(yanchor=\"top\", y=0.99, xanchor=\"left\", x=0.01),\n",
    "    )\n",
    "\n",
    "    fig = fig.update_xaxes(\n",
    "        title_text=\"Size per variant\",\n",
    "        tickvals=np.array(range(0, len(my_sizes)))[::label_every],\n",
    "        ticktext=my_sizes[::label_every]\n",
    "        / 2.0,  # divide by 2 due to sample size per variant.\n",
    "    )\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def plot_comparison_ate_pvalue(\n",
    "    cr0=0.10, crmax=0.12, sizes=[1000, 2000, 5000, 10000], one_sided=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Compares the average treatment effect (difference in conversion rates) with the P value for different sample sizes.\n",
    "    Inputs:\n",
    "    sizes: List of total sample sizes considered\n",
    "    cr0: baseline conversion rate\n",
    "    crmax: max conversion rate (we will plot ATEs until crmax - cr0)\n",
    "    Output: Plot\n",
    "    \"\"\"\n",
    "    dfs = []\n",
    "    for size in sizes:\n",
    "        pvs = []\n",
    "        ates = []\n",
    "        for cr1 in np.linspace(cr0, crmax, 100):\n",
    "            df = generate_contingency(N=size, cr0=cr0, cr1=cr1, exact=True)\n",
    "            this_ate = df.loc[1].cvr - df.loc[0].cvr\n",
    "            this_pv = p_value_binary(df, one_sided=one_sided)\n",
    "            pvs.append(this_pv)\n",
    "            ates.append(this_ate)\n",
    "        out = pd.DataFrame({\"ate\": ates, \"pvs\": pvs})\n",
    "        out[\"size\"] = size\n",
    "        dfs.append(out)\n",
    "    out_all = pd.concat(dfs)\n",
    "    fig = px.line(out_all, x=\"ate\", y=\"pvs\", color=\"size\")\n",
    "    fig.update_layout(\n",
    "        height=600,\n",
    "        width=800,\n",
    "        legend=dict(yanchor=\"top\", y=1, xanchor=\"right\", x=0.99),\n",
    "    )\n",
    "    # fig.update_xaxes(showspikes=True, spikemode=\"across\", spikethickness=1)\n",
    "    # fig.update_yaxes(showspikes=True, spikemode=\"across\", spikethickness=1)\n",
    "    # fig.update_layout(hoverdistance=100)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "# plot_comparison_ate_pvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "info = dict()\n",
    "info[\n",
    "    \"ate\"\n",
    "] = \"\"\"Average treatment effect (ate) describes the difference between the variant and the control.\n",
    "For the example for a conversion rate experiment, this is the difference between the two conversion rates of the variant and control group.\"\"\"\n",
    "info[\n",
    "    \"power\"\n",
    "] = \"\"\"Statistical power. Usually set to 0.8. This means that if the effect is the minimal effect specified above, we have an 80% probability of correctly identifying it at statistically significant (and hence 20% of not idenfitying it).\"\"\"\n",
    "info[\n",
    "    \"cr1\"\n",
    "] = \"\"\"Conversion rate variant for minimal detectable effect: cr1 (for example, if we have a conversion rate of 1% and want to detect an effect of at least 20% relate, we would set cr0=0.010 and cr1=0.012)\"\"\"\n",
    "info[\"cr0\"] = \"\"\"Conversion rate control: cr0\"\"\"\n",
    "info[\n",
    "    \"relative_uplift\"\n",
    "] = f\"\"\"Percentage of minimal detectable improvement over the base conversion rate cr0. E.g. if cr0 is 5.0%, a 10% improvement means we would observe a conversion rate of 5.5%.\"\"\"\n",
    "info[\n",
    "    \"sided\"\n",
    "] = \"\"\"As a rule of thumb, if there are very strong reasons to believe that the variant cannot be inferior to the control, we can use a one sided test. In case of doubts, using a two sided test is better.\n",
    "\"\"\"\n",
    "info[\n",
    "    \"alpha\"\n",
    "] = \"\"\"Significance threshold: alpha. Usually set to 0.05, this defines our tolerance for falsely detecting an effect if in reality there is none (alpha=0.05 means that in 5% of the cases we will detect an effect even though the samples for control and variant are drawn from the exact same distribution).\n",
    "\n",
    "\"\"\"\n",
    "info[\n",
    "    \"sigma\"\n",
    "] = \"\"\"Standard deviation (we assume the same for variant and control, should be estimated from historical data).\n",
    "\n",
    "\"\"\"\n",
    "info[\n",
    "    \"mu0\"\n",
    "] = \"\"\"Mean of the control group.\n",
    "\n",
    "\"\"\"\n",
    "info[\n",
    "    \"mu1\"\n",
    "] = \"\"\"Mean of the variant group assuming minimal detectable effect (e.g. if the mean it 5, and we want to detect an effect as small as 0.05, mu1=5.00 and mu2=5.05)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def hello():\n",
    "    print(\n",
    "        f\"\"\"\n",
    "    Welcome :) \\n\n",
    "    You can find additional documentation here: https://k111git.github.io/ab-test-toolkit/\n",
    "    \"\"\"\n",
    "    )\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Welcome :) \n",
      "\n",
      "    You can find additional documentation here: https://k111git.github.io/ab-test-toolkit/\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "hello()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
