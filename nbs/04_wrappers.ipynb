{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrappers\n",
    "\n",
    "> Wrappers to create showcases and function to explain concepts on a high level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:jupyter_black:config: {'line_length': 79, 'target_versions': {<TargetVersion.PY310: 10>}}\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            (function() {\n",
       "                jb_set_cell(\"# | export\\ninfo = dict()\\ninfo[\\n    \\\"power\\\"\\n] = \\\"\\\"\\\"Statistical power. Usually set to 0.8. This means that if the effect is the minimal effect specified above, we have an 80% probability of correctly identifying it at statistically significant (and hence 20% of not idenfitying it).\\\"\\\"\\\"\\ninfo[\\n    \\\"cr1\\\"\\n] = \\\"\\\"\\\"Conversion rate variant for minimal detectable effect: cr1 (for example, if we have a conversion rate of 1% and want to detect an effect of at least 20% relate, we would set cr0=0.010 and cr1=0.012)\\\"\\\"\\\"\\ninfo[\\\"cr0\\\"] = \\\"\\\"\\\"Conversion rate control: cr0\\\"\\\"\\\"\\ninfo[\\n    \\\"relative_uplift\\\"\\n] = f\\\"\\\"\\\"Percentage of minimal detectable improvement over the base conversion rate cr0. E.g. if cr0 is 5.0%, a 10% improvement means we would observe a conversion rate of 5.5%.\\\"\\\"\\\"\\ninfo[\\n    \\\"sided\\\"\\n] = \\\"\\\"\\\"As a rule of thumb, if there are very strong reasons to believe that the variant cannot be inferior to the control, we can use a one sided test. In case of doubts, using a two sided test is better.\\n\\\"\\\"\\\"\\ninfo[\\n    \\\"alpha\\\"\\n] = \\\"\\\"\\\"Significance threshold: alpha. Usually set to 0.05, this defines our tolerance for falsely detecting an effect if in reality there is none (alpha=0.05 means that in 5% of the cases we will detect an effect even though the samples for control and variant are drawn from the exact same distribution).\\n\\n\\\"\\\"\\\"\\ninfo[\\n    \\\"sigma\\\"\\n] = \\\"\\\"\\\"Standard deviation (we assume the same for variant and control, should be estimated from historical data).\\n\\n\\\"\\\"\\\"\\ninfo[\\n    \\\"mu0\\\"\\n] = \\\"\\\"\\\"Mean of the control group.\\n\\n\\\"\\\"\\\"\\ninfo[\\n    \\\"mu1\\\"\\n] = \\\"\\\"\\\"Mean of the variant group assuming minimal detectable effect (e.g. if the mean it 5, and we want to detect an effect as small as 0.05, mu1=5.00 and mu2=5.05)\\\"\\\"\\\"\")\n",
       "            })();\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| hide \n",
    "import black\n",
    "import jupyter_black\n",
    "\n",
    "jupyter_black.load(\n",
    "    lab=False,\n",
    "    line_length=79,\n",
    "    verbosity=\"DEBUG\",\n",
    "    target_version=black.TargetVersion.PY310,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from ab_test_toolkit.generator import (\n",
    "    generate_binary_data,\n",
    "    generate_continuous_data,\n",
    ")\n",
    "from ab_test_toolkit.power import (\n",
    "    simulate_power_binary,\n",
    "    sample_size_binary,\n",
    "    simulate_power_continuous,\n",
    "    sample_size_continuous,\n",
    ")\n",
    "from ab_test_toolkit.plotting import (\n",
    "    plot_power,\n",
    "    plot_distribution,\n",
    "    plot_betas,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def hello():\n",
    "    print(\n",
    "        f\"\"\"\n",
    "    Welcome :) \\n\n",
    "    You can find additional documentation here: https://k111git.github.io/ab-test-toolkit/\n",
    "    \"\"\"\n",
    "    )\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Welcome :) \n",
      "\n",
      "    You can find additional documentation here: https://k111git.github.io/ab-test-toolkit/\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "hello()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "info = dict()\n",
    "info[\n",
    "    \"power\"\n",
    "] = \"\"\"Statistical power. Usually set to 0.8. This means that if the effect is the minimal effect specified above, we have an 80% probability of correctly identifying it at statistically significant (and hence 20% of not idenfitying it).\"\"\"\n",
    "info[\n",
    "    \"cr1\"\n",
    "] = \"\"\"Conversion rate variant for minimal detectable effect: cr1 (for example, if we have a conversion rate of 1% and want to detect an effect of at least 20% relate, we would set cr0=0.010 and cr1=0.012)\"\"\"\n",
    "info[\"cr0\"] = \"\"\"Conversion rate control: cr0\"\"\"\n",
    "info[\n",
    "    \"relative_uplift\"\n",
    "] = f\"\"\"Percentage of minimal detectable improvement over the base conversion rate cr0. E.g. if cr0 is 5.0%, a 10% improvement means we would observe a conversion rate of 5.5%.\"\"\"\n",
    "info[\n",
    "    \"sided\"\n",
    "] = \"\"\"As a rule of thumb, if there are very strong reasons to believe that the variant cannot be inferior to the control, we can use a one sided test. In case of doubts, using a two sided test is better.\n",
    "\"\"\"\n",
    "info[\n",
    "    \"alpha\"\n",
    "] = \"\"\"Significance threshold: alpha. Usually set to 0.05, this defines our tolerance for falsely detecting an effect if in reality there is none (alpha=0.05 means that in 5% of the cases we will detect an effect even though the samples for control and variant are drawn from the exact same distribution).\n",
    "\n",
    "\"\"\"\n",
    "info[\n",
    "    \"sigma\"\n",
    "] = \"\"\"Standard deviation (we assume the same for variant and control, should be estimated from historical data).\n",
    "\n",
    "\"\"\"\n",
    "info[\n",
    "    \"mu0\"\n",
    "] = \"\"\"Mean of the control group.\n",
    "\n",
    "\"\"\"\n",
    "info[\n",
    "    \"mu1\"\n",
    "] = \"\"\"Mean of the variant group assuming minimal detectable effect (e.g. if the mean it 5, and we want to detect an effect as small as 0.05, mu1=5.00 and mu2=5.05)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
